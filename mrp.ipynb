{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TransitionStop(Generic[S]):\n",
    "    state: NonTerminal[S]\n",
    "    next_state: State[S]\n",
    "    reward: float\n",
    "\n",
    "\n",
    "class MarkovRewardProcess(MarkovProcess[S]):\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition_reward(self, state: NonTerminal[S]) -> Distribution[Tuple[State[S]], float]:\n",
    "        pass\n",
    "\n",
    "    def simulate_reward(self,\n",
    "                        start_state_distribution: Distribution[NonTerminal[S]]) -> Iterable[TransitionStep[S]]:\n",
    "        state: State[S] = start_state_distribution.sample()\n",
    "        reward: float = 0\n",
    "\n",
    "        while isinstance(state, NonTerminal):\n",
    "            next_distribution = self.trainsition_reward(state)\n",
    "            next_state, reward = next_distribution.sample()\n",
    "\n",
    "            yield TransitionStep(state, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "# create a concrete class that implement the interface of the abstract class markovRewardProcess\n",
    "# the abstractmethod transition of MarkovProcess also needs to be implmented to make the whole thing concrete\n",
    "\n",
    "def get_value_function_vec(self, gamma: float) -> np.ndarray:\n",
    "    return np.linalg.solve(\n",
    "        np.eye(len(self.nen_terminal_states)) -\n",
    "        gamma * self.get_transition_matrix(),\n",
    "        self.reward_function_vec\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#policy\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')\n",
    "\n",
    "class Policy(ABC, Generic[S, A]):\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self, state: NonTerminal[S]) -> Distribution[A]:\n",
    "        pass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DeterministicPolicy(Policy[S, A]):\n",
    "    action_for: Callable[[S],A]\n",
    "\n",
    "    def act(self, state: NonTerminal[S]) -> Constant[A]:\n",
    "        return Constant(self.action_for(state.state))\n",
    "\n",
    "from rl.distribution import Choose\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class UniformPolicy(Policy[S, A]):\n",
    "    valid_actions: Callable[[S], Iterable[A]]\n",
    "\n",
    "    def act(self, state: NonTerminal[S]) -> Choose[A]:\n",
    "        return Choose(self.valid_actions(state.state))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rl.distribution import SampledDistribution\n",
    "\n",
    "class SimpleInvertoryStochasticPolicy(Policy[InventoryState, int]):\n",
    "\n",
    "    def __init__(self, reorder_point_poisson_mean: float):\n",
    "        self.reorder_point_poisson_mean: float = reorder_point_poisson_mean\n",
    "\n",
    "    def act(self, state:NonTerminal[InventoryState]) -> SampledDistribution[int]:\n",
    "        def action_func(state=state) -> int:\n",
    "            reoder_point_sample: int = np.random.poisson(self.reorder_point_poisson_mean)\n",
    "            return max(reorder_point_sample -state.state.inventory_posiiton(), 0)\n",
    "        return SampledDistribution(action_func)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from rl.distribution import Distribution\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep(Generic[S, A]):\n",
    "    state: NonTerminal[S]\n",
    "    action: A\n",
    "    next_state: State[A]\n",
    "    reward: float\n",
    "\n",
    "class MarkovDecisionProcess(ABC, Generic[S, A]):\n",
    "    @abstractmethod\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self,\n",
    "             state:NonTerminal[S],\n",
    "             action:A ) -> Distribution[Tuple[State[S], float]]:\n",
    "        pass\n",
    "\n",
    "    def apply_policy(self, policy: Policy[S, A]) -> MarkovDecisionProcess[S]:\n",
    "        mdp=self\n",
    "\n",
    "        class RewardProcess(MarkovRewardProcess[S]):\n",
    "            def transition_reward(self,\n",
    "                                  state: Nonternimal[S]) -> Distribution[Tuple[State[S], float]]:\n",
    "                actions: Distribution[A] = policy.act(state)\n",
    "                return actions.apply(lambda a: mdp.step(state, a))\n",
    "            return RewardProcess()\n",
    "\n",
    "    def simulate_actions(self,\n",
    "                         start_states: Distribution[NonTernimal[S]],\n",
    "                         policy: Policy[S, A]) -> Iterable[TransitionStep[S,A]]:\n",
    "        state: State[S] = start_states.sample()\n",
    "\n",
    "        while isinstance(state.NonTerminal):\n",
    "            action_distribution = policy.act(state)\n",
    "\n",
    "            action = action_distribution.sample()\n",
    "            next_distribution = self.step(state, action)\n",
    "\n",
    "            next_state, reward = next_distribution.sample()\n",
    "            yield TransitionStep(state, action, next_state, reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dynamic programming\n",
    "X = TypeVar('X')\n",
    "\n",
    "def iterate(step: Callable[[X], X], start: X) -> Iterator[X]:\n",
    "    state = start\n",
    "\n",
    "    while True:\n",
    "        yield state\n",
    "        state = step(state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def converge(values: Iterator[X], done:Callable[[x,x], bool]) -> Iterator[X]:\n",
    "    a = next(values, None)\n",
    "    if a is None:\n",
    "        return\n",
    "    yield a\n",
    "\n",
    "    for b in values:\n",
    "        yield b\n",
    "        if done(a, b):\n",
    "            return\n",
    "        a = b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Policy evaluation\n",
    "DEFAULT_TOLERANCE = 1e-5\n",
    "\n",
    "V= Mapping[NonTerminal[S], float]\n",
    "\n",
    "def evaluate_mrp(\n",
    "        mrp: FiniteMarkovRewardProcess[S],\n",
    "        gamma: float\n",
    ") -> Iterator[np.ndarray]:\n",
    "    def update(v: np.ndarray) -> np.ndarray:\n",
    "        return mrp.reward_function_vec + gamma * mrp.get_transition_matrix().dot(v)\n",
    "\n",
    "    v_0: np.nd_array = np.zeros(len(mrp.non_terminal_states))\n",
    "    return iterate(update, v_0)\n",
    "\n",
    "def almost_equal_np_arrays(\n",
    "        v1: np.ndarray\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
