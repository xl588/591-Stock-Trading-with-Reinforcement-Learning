{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## Data"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2023-04-25T17:50:45.732621Z","start_time":"2023-04-25T17:50:45.728087Z"},"collapsed":false},"outputs":[],"source":["from data_simulation import process_price_traces\n","\n","start_price =  100\n","alpha = 1.0\n","time_steps = 100\n","num_traces = 500\n","\n","process_traces = process_price_traces(\n","    start_price=start_price,\n","    alpha=alpha,\n","    time_steps=time_steps,\n","    num_traces=num_traces\n",")"]},{"cell_type":"markdown","metadata":{"collapsed":false},"source":["## Modeling"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2023-04-25T17:20:53.150635Z","start_time":"2023-04-25T17:20:47.732165Z"},"collapsed":false},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-08 16:56:31.327252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["from keras.models import Sequential\n","from keras.models import load_model\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","import math\n","import numpy as np\n","import random\n","import yfinance as yf\n","from collections import deque"]},{"cell_type":"code","execution_count":118,"metadata":{"ExecuteTime":{"end_time":"2023-04-26T12:25:18.808312Z","start_time":"2023-04-26T12:25:18.798567Z"},"collapsed":false},"outputs":[],"source":["class Agent:\n","        def __init__(self, state_size, is_eval=False):\n","            self.state_size = state_size \n","            self.action_size = 3 # hold, buy, sell\n","            self.memory = deque(maxlen=1000)\n","            self.inventory = []\n","            self.is_eval = is_eval\n","            self.gamma = 0.95\n","            self.epsilon = 1.0\n","            self.epsilon_min = 0.01\n","            self.epsilon_decay = 0.995\n","            self.model = self._model()\n","            self.loss_l = []\n","            self.val_loss_l = []\n","\n","        def _model(self):\n","            model = Sequential()\n","            model.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n","            model.add(Dense(units=32, activation=\"relu\"))\n","            model.add(Dense(units=8, activation=\"relu\"))\n","            model.add(Dense(self.action_size, activation=\"linear\"))\n","            model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n","            return model\n","\n","        def act(self, state):\n","            if not self.is_eval and random.random()<= self.epsilon:\n","                return random.randrange(self.action_size)\n","            options = self.model.predict(state, verbose=0)\n","            return np.argmax(options[0])\n","\n","        def optimize(self, batch_size):\n","            mini_batch = []\n","            l = len(self.memory)\n","            for i in range(l - batch_size + 1, l):\n","                mini_batch.append(self.memory[i])\n","            for state, action, reward, next_state, done in mini_batch:\n","                target = reward\n","                if not done:\n","                    target = reward + self.gamma * np.amax(self.model.predict(next_state,verbose=0)[0])\n","                target_f = self.model.predict(state, verbose=0)\n","                target_f[0][action] = target\n","                train_history = self.model.fit(state, target_f, epochs=1, verbose=0)\n","                self.loss_l.append(train_history.history['loss'])\n","                #self.val_loss_l.append(train_history.history['val_loss'])\n","            if self.epsilon > self.epsilon_min:\n","                self.epsilon *= self.epsilon_decay"]},{"cell_type":"code","execution_count":88,"metadata":{"ExecuteTime":{"end_time":"2023-04-25T23:56:53.675147Z","start_time":"2023-04-25T23:56:53.669563Z"},"collapsed":false},"outputs":[],"source":["def formatPrice(n):\n","    return(\"-Rs.\" if n<0 else \"Rs.\")+\"{0:.2f}\".format(abs(n))\n","\n","def sigmoid(x):\n","    return 1/(1+math.exp(-x))\n","\n","def getState(data, t, n):\n","\n","    if t < 5:\n","        window = -(t-n+1)*[data[0]] + list(data[0: t+1])\n","    else:\n","        window = data[t-n+1:t + 1]\n","\n","    return np.array(window).reshape((1, n))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sp500 = yf.download(\"^GSPC\", start=\"2021-05-8\", end=\"2023-05-7\", interval=\"1h\")\n","sp500 = sp500['Adj Close'] - sp500['Open']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = sp500.values # np.ndarray\n","window_size = 5 # state space size, consist with previous window_size days\n","agent = Agent(window_size)\n","num_episodes = 100\n","l = len(data) - 1\n","batch_size = 32\n","t_p = []\n","for i_episode in range(num_episodes + 1):\n","    print(\"Episode \" + str(i_episode) + \"/\" + str(num_episodes))\n","    state = getState(data, 0, window_size)\n","    total_profit = 0\n","    agent.inventory = []\n","    for t in range(l):\n","        action = agent.act(state)\n","        # sit\n","        next_state = getState(data, t + 1, window_size)#) + 1)\n","        reward = 0\n","        if action == 1: # buy\n","            agent.inventory.append(data[t])\n","        elif action == 2 and len(agent.inventory) > 0: # sell\n","            bought_price = window_size_price = agent.inventory.pop(0)\n","            # positive income from the transactions or 0\n","            reward = max(data[t] - bought_price, 0)\n","            # cumulative profit for the episode\n","            total_profit += data[t] - bought_price\n","        done = True if t == l - 1 else False\n","        agent.memory.append((state, action, reward, next_state, done))\n","        state = next_state\n","        if done:\n","            t_p.append(total_profit)\n","    if len(agent.memory) > batch_size:\n","        agent.optimize(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# DEFINE YOUR VISUALIZATION\n","import matplotlib.pyplot as plt\n","fig, axs = plt.subplots(1,1, figsize=(6, 6),)\n","\n","#fig.suptitle(\"Total profits over episodes\", fontsize=15)\n","fig.tight_layout(pad=2)\n","\n","axs.plot(range(len(t_p)), t_p,)\n","axs.set_title(\"Total profits over episodes\")\n","axs.set_xlabel(\"Episode\")\n","axs.set_ylabel(\"Total profit\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = process_traces # np.ndarray\n","window_size = 100 # state space size, consist with previous window_size days\n","agent = Agent(window_size)\n","num_episodes = 1000\n","l = len(data) - 101\n","batch_size = 32\n","t_p = []\n","for i_episode in range(num_episodes):\n","    print(\"Episode \" + str(i_episode) + \"/\" + str(num_episodes))\n","    state = getState(data[i_episode], 0, window_size)\n","    total_profit = 0\n","    agent.inventory = []\n","    for t in range(l):\n","        action = agent.act(state)\n","        # sit\n","        next_state = getState(data[i_episode], t + 1, window_size)#) + 1)\n","        reward = 0\n","        if action == 1: # buy\n","            agent.inventory.append(data[i_episode][t])\n","        elif action == 2 and len(agent.inventory) > 0: # sell\n","            bought_price = window_size_price = agent.inventory.pop(0)\n","            # positive income from the transactions or 0\n","            reward = max(data[i_episode][t] - bought_price, 0)\n","            # cumulative profit for the episode\n","            total_profit += data[i_episode][t] - bought_price\n","        done = True if t == l - 1 else False\n","        agent.memory.append((state, action, reward, next_state, done))\n","        state = next_state\n","        if done:\n","            t_p.append(total_profit)\n","    if i_episode % 50 == 0:\n","        if len(agent.memory) > batch_size:\n","            agent.optimize(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(1,1, figsize=(6, 6),)\n","\n","#fig.suptitle(\"Total profits over episodes\", fontsize=15)\n","fig.tight_layout(pad=2)\n","\n","axs.plot(range(len(t_p)), t_p,)\n","axs.set_title(\"Total profits over episodes\")\n","axs.set_xlabel(\"Episode\")\n","axs.set_ylabel(\"Total profit\")"]},{"cell_type":"code","execution_count":148,"metadata":{"ExecuteTime":{"end_time":"2023-04-26T13:37:24.999351Z","start_time":"2023-04-26T13:37:24.988245Z"},"collapsed":false},"outputs":[],"source":["vector_norm = agent.loss_l/np.linalg.norm(agent.loss_l)\n","fig, axs = plt.subplots(1,1, figsize=(6, 6),)\n","\n","fig.tight_layout(pad=2)\n","\n","axs.plot(np.array(range(4500)).astype(int), vector_norm[:4500])#list(np.array(agent.loss_l)),)\n","axs.set_title(\"Normalized Training losses over episodes\")\n","axs.set_xlabel(\"Fits\")\n","axs.set_ylabel(\"Training Loss\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.9 ('TF')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"vscode":{"interpreter":{"hash":"3a7a2212f45f3f63f6257f819ae341897101d4b199861a5deb5ef5579823a8a2"}}},"nbformat":4,"nbformat_minor":0}
